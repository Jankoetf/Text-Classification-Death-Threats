{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# reloading Loader Class\n",
    "import DatasetManagment.dataset_loader\n",
    "importlib.reload(DatasetManagment.dataset_loader)\n",
    "from DatasetManagment.dataset_loader import DatasetLoader\n",
    "\n",
    "#reloading Loader constants\n",
    "import DatasetManagment.dataset_const\n",
    "importlib.reload(DatasetManagment.dataset_const)\n",
    "from DatasetManagment.dataset_const import (DATASETS_ROOT, TEST_DATASETS_ROOT)\n",
    "\n",
    "#reloading augmentation class\n",
    "import DatasetManagment.augmentation\n",
    "importlib.reload(DatasetManagment.augmentation)\n",
    "from DatasetManagment.augmentation import DatasetAugmentation\n",
    "\n",
    "#reloading keyboard_neighborhood\n",
    "import DatasetManagment.keyboard_neighborhood\n",
    "importlib.reload(DatasetManagment.keyboard_neighborhood)\n",
    "from DatasetManagment.keyboard_neighborhood import keyboard_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming\n",
    "sheet_name_default = \"All\"; sheets_names = [sheet_name_default]\n",
    "initial_manual_dataset_name = \"TestInitial\"\n",
    "augmented_dataset_name = \"TestAugmented\"\n",
    "removed_duplicates_dataset_name = \"TestRemovedDuplicates\"\n",
    "with_noise_dataset_name = \"TestAddedNoise\"\n",
    "\n",
    "loader_instance = DatasetLoader() # used for loading excel sheets\n",
    "augmentation_instance = DatasetAugmentation(keyboard_neighborhood, \"gpt-4o\", TEST_DATASETS_ROOT) # dataset augmentaion, TEST_DATASETS_ROOT is root\n",
    "\n",
    "#paths\n",
    "initial_manual_dataset_path = augmentation_instance.path_from_name(initial_manual_dataset_name)\n",
    "augmented_dataset_path = augmentation_instance.path_from_name(augmented_dataset_name)\n",
    "removed_duplicates_dataset_path = augmentation_instance.path_from_name(removed_duplicates_dataset_name)\n",
    "with_noise_dataset_path = augmentation_instance.path_from_name(with_noise_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  38 1:  34\n",
      "!!!!!!!!!! not balanced\n"
     ]
    }
   ],
   "source": [
    "# loading initial manually created dataset\n",
    "initial_manual_dataset = loader_instance.add_all_sheets_from_all_files([initial_manual_dataset_path], [sheets_names])\n",
    "\n",
    "hyperparams = {\"n_responses_per_request\":1, \"list_of_temperatures\":[0.8]} # hyperparams for testing\n",
    "\n",
    "# if hyperparams = None -> hyperparams = {\"n_responses_per_request\":2, \"list_of_temperatures\":[0.75, 0.85, 0.95]}\n",
    "augmentation_instance.new_dataset_by_paraphrazing(initial_manual_dataset, augmented_dataset_name, hyperparams=hyperparams) # creating new dataset with paraphrazing using OPEN AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  76 1:  68\n",
      "!!!!!!!!!! not balanced\n"
     ]
    }
   ],
   "source": [
    "# augmented_dataset needs to be cleaned manually - in this example there is no manual cleaning\n",
    "augmented_dataset = loader_instance.add_all_sheets_from_all_files([augmented_dataset_path], [sheets_names])\n",
    "augmentation_instance.filter_duplicates(augmented_dataset_path, removed_duplicates_dataset_name) # lets remove duplicates if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  76 1:  68\n",
      "!!!!!!!!!! not balanced\n",
      "0:  78 1:  71\n",
      "!!!!!!!!!! not balanced\n"
     ]
    }
   ],
   "source": [
    "# loading filtered dataset\n",
    "removed_duplicates_dataset = loader_instance.add_all_sheets_from_all_files([removed_duplicates_dataset_path], [sheets_names])\n",
    "\n",
    "# adding noise - typos\n",
    "augmentation_instance.new_dataset_by_adding_noise(augmented_dataset, with_noise_dataset_name)\n",
    "\n",
    "# final dataset, testing dataset preprocessing finished\n",
    "with_noise_dataset = loader_instance.add_all_sheets_from_all_files([with_noise_dataset_path], [sheets_names])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
